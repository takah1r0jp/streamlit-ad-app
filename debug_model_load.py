#!/usr/bin/env python3
"""
Grounding DINOãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
EC2ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æˆåŠŸ/å¤±æ•—ã‚’ç¢ºèª
"""

import psutil
import torch
import time
import gc
from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection

def print_memory_usage(stage):
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¡¨ç¤º"""
    memory = psutil.virtual_memory()
    print(f"[{stage}] Available RAM: {memory.available / (1024**3):.2f} GB")
    print(f"[{stage}] Used RAM: {(memory.total - memory.available) / (1024**3):.2f} GB")
    print(f"[{stage}] Memory usage: {memory.percent}%")
    print("-" * 50)

def main():
    print("=== Grounding DINO Model Load Test ===")
    print_memory_usage("Initial")
    
    print(f'PyTorch version: {torch.__version__}')
    print(f'CUDA available: {torch.cuda.is_available()}')
    print(f'CUDA device count: {torch.cuda.device_count() if torch.cuda.is_available() else 0}')
    
    if torch.cuda.is_available():
        print(f'CUDA device name: {torch.cuda.get_device_name(0)}')
    
    print("-" * 50)
    
    try:
        model_id = 'IDEA-Research/grounding-dino-base'
        print(f'Model ID: {model_id}')
        print('Starting model load test...')
        
        # Step 1: ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ­ãƒ¼ãƒ‰
        print('\n1. Loading processor...')
        start_time = time.time()
        processor = AutoProcessor.from_pretrained(model_id)
        processor_time = time.time() - start_time
        print(f'Processor loaded successfully! ({processor_time:.2f}s)')
        print_memory_usage("After Processor Load")
        
        # Step 2: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        print('\n2. Loading model...')
        start_time = time.time()
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f'Using device: {device}')
        
        model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id)
        model = model.to(device)
        model_time = time.time() - start_time
        
        print(f'Model loaded successfully! ({model_time:.2f}s)')
        print_memory_usage("After Model Load")
        
        # Step 3: ç°¡å˜ãªæ¨è«–ãƒ†ã‚¹ãƒˆ
        print('\n3. Testing inference...')
        from PIL import Image
        import numpy as np
        
        # ãƒ€ãƒŸãƒ¼ç”»åƒä½œæˆ
        dummy_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
        
        start_time = time.time()
        inputs = processor(images=dummy_image, text="apple.", return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        inference_time = time.time() - start_time
        print(f'Inference test completed! ({inference_time:.2f}s)')
        print_memory_usage("After Inference")
        
        # Step 4: ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ
        print('\n4. Testing memory cleanup...')
        del model
        del processor
        del inputs
        del outputs
        gc.collect()
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print_memory_usage("After Cleanup")
        
        print("\nâœ… ALL TESTS PASSED!")
        
    except Exception as e:
        print(f'\nâŒ Model loading failed: {e}')
        print("\nDetailed error:")
        import traceback
        traceback.print_exc()
        
        print_memory_usage("After Error")
        
        # ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆã®å¯¾ç­–ææ¡ˆ
        if "out of memory" in str(e).lower() or "memory" in str(e).lower():
            print("\nğŸ”§ MEMORY ISSUE DETECTED!")
            print("Suggested solutions:")
            print("1. Increase EC2 instance to t3.medium (4GB RAM)")
            print("2. Add swap memory: sudo swapon /swapfile") 
            print("3. Use smaller model: grounding-dino-tiny")

if __name__ == "__main__":
    main()